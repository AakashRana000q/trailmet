{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This notebook describes how to load CIFAR10 or CIFAR100 datasets using the trailmet framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of trailmet\n",
    "Note that if the source code version of trailmet is used, then the path to the root directory needs to be added to the system path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding trailmet to the system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/deepak.gupta/eff-dl/trailmet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from trailmet.datasets.classification import DatasetFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the root data directory\n",
    "This directory will be used to download (and process if needed) the required data. Any folders related to train/test etc. will be created at this path. Please adapt this path based on your system's address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/deepak.gupta/eff-dl/data_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify the transforms to be applied on the inputs and labels of the train, val and test splits\n",
    "All the transforms to be applied on the different splits of the data can be specified using transforms function from the torchvision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "input_transforms = {\n",
    "    'train': train_transform, \n",
    "    'val': val_transform, \n",
    "    'test': test_transform}\n",
    "\n",
    "target_transforms = {\n",
    "    'train': None, \n",
    "    'val': None, \n",
    "    'test': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the CIFAR10 dataset with the specified control parameters\n",
    " - val_fraction defines the fraction of the training data from the original CIFAR10 to be seperated as validation set. Note that the test is preserved to be the same as the standard one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = DatasetFactory.create_dataset(name = 'CIFAR10', \n",
    "                                        root = root_dir,\n",
    "                                        split_types = ['train', 'val2', 'test'],\n",
    "                                        val_fraction = 0.2,\n",
    "                                        transform = input_transforms,\n",
    "                                        target_transform = target_transforms\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'val_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'test_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'train_sampler': <torch.utils.data.sampler.SubsetRandomSampler object at 0x7fac601f8fa0>, 'val_sampler': <torch.utils.data.sampler.SubsetRandomSampler object at 0x7fac91794a00>, 'test_sampler': None}\n"
     ]
    }
   ],
   "source": [
    "print(cifar_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the created dataset object are described above. The whole dataset is divided into three parts: train_dataset, val_dataset and test_dataset, and these names are used as keys of a dictionary to access the sub-objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        cifar_dataset['train_dataset'], batch_size=64, \n",
    "        sampler=cifar_dataset['train_sampler'],\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[7,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "967adbfb1ca2aceff7a289bcfc89c2ce7530ccd9a423fa20d67f11f0c8d2c1b4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
