{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "This notebook describes how to load CIFAR10 or CIFAR100 datasets using the trailmet framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of trailmet\n",
    "Note that if the source code version of trailmet is used, then the path to the root directory needs to be added to the system path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding trailmet to the system path\n",
    "import sys\n",
    "sys.path.append(\"/Users/deepak.gupta/eff-dl/trailmet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required packages\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from trailmet.datasets.classification import DatasetFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the root data directory\n",
    "This directory will be used to download (and process if needed) the required data. Any folders related to train/test etc. will be created at this path. Please adapt this path based on your system's address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/Users/deepak.gupta/eff-dl/data_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify the transforms to be applied on the inputs and labels of the train, val and test splits\n",
    "All the transforms to be applied on the different splits of the data can be specified using transforms function from the torchvision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "[transforms.ToTensor()])\n",
    "\n",
    "input_transforms = {\n",
    "    'train': train_transform, \n",
    "    'val': val_transform, \n",
    "    'test': test_transform}\n",
    "\n",
    "target_transforms = {\n",
    "    'train': None, \n",
    "    'val': None, \n",
    "    'test': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the CIFAR10 dataset with the specified control parameters\n",
    " - val_fraction defines the fraction of the training data from the original CIFAR10 to be seperated as validation set. Note that the test is preserved to be the same as the standard one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'int' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cifar_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCIFAR10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msplit_types\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mval_fraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_transforms\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eff-dl/trailmet/trailmet/datasets/classification/__init__.py:29\u001b[0m, in \u001b[0;36mDatasetFactory.create_dataset\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould provide dataset root\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR10\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m name:\n\u001b[0;32m---> 29\u001b[0m     obj_dfactory \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR10Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m  obj_dfactory\u001b[38;5;241m.\u001b[39mstack_dataset()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR100\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m==\u001b[39m name:\n",
      "File \u001b[0;32m~/eff-dl/trailmet/trailmet/datasets/classification/cifar.py:75\u001b[0m, in \u001b[0;36mCIFAR10Dataset.__init__\u001b[0;34m(self, name, root, transform, target_transform, download, split_types, val_fraction, shuffle, random_seed)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_fraction needs to be of float type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_fraction \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_fraction\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_fraction should be in range [0.0, 1.0]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#     raise Exception(self.val_fraction >= 0 & self.val_fraction < 1.0)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# except TypeError:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m## To do: chcek val_frac is float, else raise error\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m## To do: if shuffle is true, there should be 'val' in train test split\u001b[39;00m\n\u001b[1;32m     87\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mCIFAR10\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'int' and 'float'"
     ]
    }
   ],
   "source": [
    "cifar_dataset = DatasetFactory.create_dataset(name = 'CIFAR10', \n",
    "                                        root = root_dir,\n",
    "                                        split_types = ['train', 'val', 'test'],\n",
    "                                        val_fraction = 1.1,\n",
    "                                        transform = input_transforms,\n",
    "                                        target_transform = target_transforms\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'val_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'test_dataset': Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: /Users/deepak.gupta/eff-dl/data_dir\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           ), 'train_sampler': <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f9f60a6fa90>, 'val_sampler': <torch.utils.data.sampler.SubsetRandomSampler object at 0x7f9f58622580>, 'test_sampler': None}\n"
     ]
    }
   ],
   "source": [
    "print(cifar_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the created dataset object are described above. The whole dataset is divided into three parts: train_dataset, val_dataset and test_dataset, and these names are used as keys of a dictionary to access the sub-objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        cifar_dataset['train_dataset'], batch_size=64, \n",
    "        sampler=cifar_dataset['train_sampler'],\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[7,0,:,:].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "967adbfb1ca2aceff7a289bcfc89c2ce7530ccd9a423fa20d67f11f0c8d2c1b4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
